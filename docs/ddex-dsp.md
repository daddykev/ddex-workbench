# DDEX DSP - Blueprint

## Project Overview

DDEX DSP is an open-source, npm-installable Digital Service Provider (streaming platform) that democratizes music streaming technology by providing a complete, DDEX-compliant platform anyone can deploy in minutes. Whether you're a distributor testing feeds, a startup entering new markets, or a brand launching a curated streaming experience, DDEX DSP handles the entire pipeline from ingesting DDEX ERN deliveries to delivering a world-class listening experience. Built as the consumption endpoint of the DDEX Workbench ecosystem alongside DDEX Distro (distribution).

### Vision
Transform streaming platform creation from a multi-million dollar endeavor to a single npm command, enabling innovation in underserved markets and use cases while maintaining enterprise-grade DDEX compliance and **providing the most flexible ingestion options in the industry**.

### Core Value Propositions
- **Instant Streaming Platform**: Deploy a functional DSP service with one command
- **DDEX-Native Ingestion**: Built to receive and process ERN deliveries seamlessly
- **Multi-Protocol Ingestion**: Support FTP, SFTP, Amazon S3, and Google Cloud Storage delivery methods
- **Complete Streaming Stack**: Catalog, search, playback, and user management included
- **Test Environment Ready**: Perfect for developers to test their DDEX Distro deployments with multiple protocols
- **White-Label Capable**: Fully customizable for any brand or market

## Technical Architecture

### Platform Stack
- **Frontend**: Vue 3 (Composition API) + Vite
- **Backend**: Firebase (Firestore, Functions, Storage, Auth)
- **Ingestion Servers**: Node.js FTP/SFTP servers + Cloud Storage listeners (S3/GCS)
- **Streaming**: Firebase Storage + CDN with adaptive bitrate
- **Search**: Algolia or Typesense integration
- **Processing**: Cloud Functions for ERN processing
- **Analytics**: Firebase Analytics + custom DSR generation
- **CLI**: Node.js CLI for project scaffolding

### Enhanced Ingestion Architecture

#### Multi-Protocol Support
```javascript
// Supported delivery protocols
const DeliveryProtocols = {
  FTP: {
    server: 'ftp.my-dsp.com',
    port: 21,
    authType: 'username/password'
  },
  SFTP: {
    server: 'sftp.my-dsp.com', 
    port: 22,
    authType: 'ssh-key or password'
  },
  S3: {
    bucket: 'my-dsp-deliveries',
    region: 'us-east-1',
    authType: 'iam-role, access-keys, or signed-url'
  },
  GoogleCloudStorage: {
    bucket: 'deliveries-my-dsp',
    authType: 'service-account or signed-url'
  }
}
```

### Deployment Model
```bash
# One-command deployment with protocol selection
npx create-ddex-dsp my-streaming-service \
  --protocols=ftp,sftp,s3,gcs \
  --template=streaming

cd my-streaming-service
npm run deploy
```

## Project Structure (Enhanced)

```
ddex-dsp/
â”œâ”€â”€ cli/                          # CLI tool for scaffolding
â”‚   â”œâ”€â”€ bin/                      
â”‚   â”‚   â””â”€â”€ ddex-dsp.js           
â”‚   â”œâ”€â”€ commands/                 
â”‚   â”‚   â”œâ”€â”€ create.js             
â”‚   â”‚   â”œâ”€â”€ init.js               
â”‚   â”‚   â”œâ”€â”€ deploy.js             
â”‚   â”‚   â”œâ”€â”€ configure.js          
â”‚   â”‚   â””â”€â”€ protocols.js          # Protocol configuration
â”‚   â”œâ”€â”€ templates/                
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ packages/                     
â”‚   â”œâ”€â”€ @ddex/dsp-core/           
â”‚   â”œâ”€â”€ @ddex/player/             
â”‚   â”œâ”€â”€ @ddex/storefront/         
â”‚   â””â”€â”€ @ddex/ingestion-servers/  # Protocol servers
â”‚       â”œâ”€â”€ src/
â”‚       â”‚   â”œâ”€â”€ ftp/              # FTP server implementation
â”‚       â”‚   â”‚   â”œâ”€â”€ server.js     # FTP server setup
â”‚       â”‚   â”‚   â”œâ”€â”€ auth.js       # Authentication
â”‚       â”‚   â”‚   â””â”€â”€ handlers.js   # File handlers
â”‚       â”‚   â”œâ”€â”€ sftp/             # SFTP server implementation
â”‚       â”‚   â”‚   â”œâ”€â”€ server.js     # SFTP server setup
â”‚       â”‚   â”‚   â”œâ”€â”€ auth.js       # SSH key/password auth
â”‚       â”‚   â”‚   â””â”€â”€ handlers.js   # File handlers
â”‚       â”‚   â”œâ”€â”€ s3/               # Amazon S3 implementation
â”‚       â”‚   â”‚   â”œâ”€â”€ listener.js   # S3 event processor
â”‚       â”‚   â”‚   â”œâ”€â”€ auth.js       # IAM/access key auth
â”‚       â”‚   â”‚   â”œâ”€â”€ handlers.js   # Object handlers
â”‚       â”‚   â”‚   â””â”€â”€ lambda.js     # Lambda function entry
â”‚       â”‚   â”œâ”€â”€ gcs/              # Google Cloud Storage
â”‚       â”‚   â”‚   â”œâ”€â”€ listener.js   # GCS event listener
â”‚       â”‚   â”‚   â”œâ”€â”€ auth.js       # Service account auth
â”‚       â”‚   â”‚   â””â”€â”€ handlers.js   # Object handlers
â”‚       â”‚   â””â”€â”€ common/           # Shared ingestion logic
â”‚       â”‚       â”œâ”€â”€ processor.js  # ERN processing
â”‚       â”‚       â”œâ”€â”€ validator.js  # Validation
â”‚       â”‚       â””â”€â”€ notifier.js   # Acknowledgments
â”‚       â””â”€â”€ package.json
â”œâ”€â”€ template/                     
â”‚   â”œâ”€â”€ src/                      
â”‚   â”‚   â”œâ”€â”€ components/           
â”‚   â”‚   â”‚   â”œâ”€â”€ admin/            
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ DeliveryConfig.vue    # Protocol config UI
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ DeliveryMonitor.vue   # Real-time monitoring
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ProtocolTest.vue      # Test deliveries
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Deliveries.vue
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Catalog.vue
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Analytics.vue
â”‚   â”‚   â”œâ”€â”€ views/                
â”‚   â”‚   â”œâ”€â”€ stores/               
â”‚   â”‚   â”‚   â”œâ”€â”€ delivery.js       # Delivery configuration store
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ services/             
â”‚   â”‚   â”‚   â”œâ”€â”€ protocols.js      # Protocol management API
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ functions/                
â”‚   â”‚   â”œâ”€â”€ ingestion/            # Enhanced ingestion
â”‚   â”‚   â”‚   â”œâ”€â”€ receiver.js       # Multi-protocol receiver
â”‚   â”‚   â”‚   â”œâ”€â”€ protocols/        # Protocol-specific handlers
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ftp.js        # FTP ingestion handler
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ sftp.js       # SFTP ingestion handler
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ s3.js         # S3 ingestion handler
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ gcs.js        # GCS ingestion handler
â”‚   â”‚   â”‚   â”œâ”€â”€ parser.js         
â”‚   â”‚   â”‚   â”œâ”€â”€ validator.js      
â”‚   â”‚   â”‚   â”œâ”€â”€ processor.js      
â”‚   â”‚   â”‚   â””â”€â”€ notifier.js       
â”‚   â”‚   â”œâ”€â”€ servers/              # Protocol server management
â”‚   â”‚   â”‚   â”œâ”€â”€ ftp-manager.js    # FTP server lifecycle
â”‚   â”‚   â”‚   â”œâ”€â”€ sftp-manager.js   # SFTP server lifecycle
â”‚   â”‚   â”‚   â”œâ”€â”€ s3-manager.js     # S3 configuration manager
â”‚   â”‚   â”‚   â””â”€â”€ credentials.js    # Secure credential storage
â”‚   â”œâ”€â”€ lambda/                   # AWS Lambda functions
â”‚   â”‚   â”œâ”€â”€ s3-ingestion/         
â”‚   â”‚   â”‚   â”œâ”€â”€ index.js          # Lambda handler
â”‚   â”‚   â”‚   â”œâ”€â”€ package.json      # Lambda dependencies
â”‚   â”‚   â”‚   â””â”€â”€ serverless.yml    # Serverless config
â”‚   â”œâ”€â”€ servers/                  # Standalone protocol servers
â”‚   â”‚   â”œâ”€â”€ ftp/                  
â”‚   â”‚   â”‚   â”œâ”€â”€ Dockerfile        # FTP server container
â”‚   â”‚   â”‚   â”œâ”€â”€ server.js         # FTP server entry
â”‚   â”‚   â”‚   â””â”€â”€ config.js         # FTP configuration
â”‚   â”‚   â”œâ”€â”€ sftp/                 
â”‚   â”‚   â”‚   â”œâ”€â”€ Dockerfile        # SFTP server container
â”‚   â”‚   â”‚   â”œâ”€â”€ server.js         # SFTP server entry
â”‚   â”‚   â”‚   â””â”€â”€ config.js         # SFTP configuration
â”‚   â”‚   â””â”€â”€ docker-compose.yml    # Multi-server orchestration
â”‚   â”œâ”€â”€ scripts/                  
â”‚   â”‚   â”œâ”€â”€ setup-protocols.js    # Protocol setup wizard
â”‚   â”‚   â”œâ”€â”€ deploy-lambda.js      # Lambda deployment script
â”‚   â”‚   â””â”€â”€ test-delivery.js      # Test delivery script
```

## Enhanced Core Features

### 1. Multi-Protocol ERN Ingestion Pipeline

#### Delivery Feed Configuration
```typescript
interface DeliveryFeed {
  id: string;
  name: string;
  distributorId: string;
  protocol: 'FTP' | 'SFTP' | 'S3' | 'GCS';
  
  config: {
    // FTP Configuration
    ftp?: {
      host: string;
      port: number;
      username: string;
      password: encrypted;
      directory: string;
      passive: boolean;
      secure: boolean; // FTPS
    };
    
    // SFTP Configuration  
    sftp?: {
      host: string;
      port: number;
      username: string;
      authentication: {
        type: 'password' | 'key';
        password?: encrypted;
        privateKey?: encrypted;
        passphrase?: encrypted;
      };
      directory: string;
    };
    
    // Amazon S3 Configuration
    s3?: {
      bucket: string;
      region: string;
      prefix: string;
      authentication: {
        type: 'iam-role' | 'access-keys' | 'signed-url';
        accessKeyId?: encrypted;
        secretAccessKey?: encrypted;
        roleArn?: string;
        externalId?: string; // For cross-account access
      };
      eventConfiguration?: {
        lambdaArn: string;
        snsTopicArn?: string;
        sqsQueueUrl?: string;
      };
    };
    
    // Google Cloud Storage Configuration
    gcs?: {
      bucket: string;
      prefix: string;
      serviceAccount?: encrypted;
      signedUrlEndpoint?: string;
    };
  };
  
  processing: {
    autoProcess: boolean;
    schedule?: CronSchedule;
    retryPolicy: RetryPolicy;
    notifications: NotificationConfig;
  };
  
  status: {
    active: boolean;
    lastDelivery?: Timestamp;
    lastError?: string;
    statistics: DeliveryStats;
  };
}
```

#### FTP Server Implementation
```javascript
// FTP server with Firebase integration
import { FtpSrv } from 'ftp-srv';

class DDEXFTPServer {
  constructor(config) {
    this.server = new FtpSrv({
      url: `ftp://${config.host}:${config.port}`,
      pasv_url: config.pasvUrl,
      greeting: 'DDEX DSP FTP Server Ready'
    });
    
    this.setupAuthentication();
    this.setupHandlers();
  }
  
  setupAuthentication() {
    this.server.on('login', async ({ username, password }, resolve, reject) => {
      // Validate against delivery feeds
      const feed = await validateFeedCredentials(username, password);
      
      if (feed) {
        const fileSystem = new DDEXFileSystem(feed);
        resolve({ root: feed.directory, fs: fileSystem });
      } else {
        reject(new Error('Invalid credentials'));
      }
    });
  }
  
  setupHandlers() {
    // Handle file uploads
    this.server.on('upload', async (error, { path, stream }) => {
      if (path.endsWith('.xml')) {
        // Process ERN file
        await processERNDelivery(stream, path);
      } else {
        // Store asset file
        await storeAssetFile(stream, path);
      }
    });
  }
}
```

#### SFTP Server Implementation
```javascript
// SFTP server with SSH key authentication
import { Server } from 'ssh2';

class DDEXSFTPServer {
  constructor(config) {
    this.server = new Server({
      hostKeys: [config.hostKey],
      greeting: 'DDEX DSP SFTP Server'
    });
    
    this.setupAuthentication();
    this.setupSFTPHandlers();
  }
  
  setupAuthentication() {
    this.server.on('authentication', async (ctx) => {
      const feed = await findDeliveryFeed(ctx.username);
      
      if (!feed) {
        return ctx.reject();
      }
      
      // Support both password and key authentication
      if (ctx.method === 'password') {
        const valid = await validatePassword(feed, ctx.password);
        valid ? ctx.accept() : ctx.reject();
      } else if (ctx.method === 'publickey') {
        const valid = await validatePublicKey(feed, ctx.key);
        valid ? ctx.accept() : ctx.reject();
      }
    });
  }
  
  setupSFTPHandlers() {
    this.server.on('session', (accept) => {
      const session = accept();
      
      session.on('sftp', (accept) => {
        const sftp = accept();
        
        // Handle file operations
        sftp.on('WRITE', async (reqId, handle, offset, data) => {
          await handleFileWrite(handle, offset, data);
          sftp.status(reqId, STATUS_CODE.OK);
        });
        
        sftp.on('CLOSE', async (reqId, handle) => {
          const file = await closeFile(handle);
          if (file.name.endsWith('.xml')) {
            await processERNDelivery(file);
          }
          sftp.status(reqId, STATUS_CODE.OK);
        });
      });
    });
  }
}
```

#### Amazon S3 Implementation
```javascript
// S3 listener with Lambda integration
import { S3Client, GetObjectCommand, PutObjectCommand } from '@aws-sdk/client-s3';
import { getSignedUrl } from '@aws-sdk/s3-request-presigner';

class S3DeliveryListener {
  constructor(config) {
    this.s3Client = new S3Client({ 
      region: config.region,
      credentials: this.getCredentials(config)
    });
    this.bucket = config.bucket;
    this.prefix = config.prefix;
  }
  
  getCredentials(config) {
    switch(config.authentication.type) {
      case 'access-keys':
        return {
          accessKeyId: decrypt(config.authentication.accessKeyId),
          secretAccessKey: decrypt(config.authentication.secretAccessKey)
        };
      case 'iam-role':
        // Use assumed role or instance profile
        return fromTemporaryCredentials({
          params: {
            RoleArn: config.authentication.roleArn,
            ExternalId: config.authentication.externalId
          }
        });
      default:
        // Use default credentials chain
        return undefined;
    }
  }
  
  // Lambda handler for S3 events
  async handleS3Event(event) {
    console.log('Processing S3 event:', JSON.stringify(event));
    
    for (const record of event.Records) {
      const bucket = record.s3.bucket.name;
      const key = decodeURIComponent(record.s3.object.key.replace(/\+/g, ' '));
      
      // Skip if not in our prefix
      if (!key.startsWith(this.prefix)) {
        console.log(`Skipping ${key} - outside prefix ${this.prefix}`);
        continue;
      }
      
      // Identify the feed from the path
      const feed = await this.identifyFeedFromPath(key);
      if (!feed) {
        console.error(`No feed found for path: ${key}`);
        continue;
      }
      
      // Process based on file type
      if (key.endsWith('.xml')) {
        await this.processERN(bucket, key, feed);
      } else {
        await this.processAsset(bucket, key, feed);
      }
    }
  }
  
  async processERN(bucket, key, feed) {
    try {
      // Download the ERN file
      const command = new GetObjectCommand({
        Bucket: bucket,
        Key: key
      });
      
      const response = await this.s3Client.send(command);
      const ernContent = await streamToString(response.Body);
      
      // Process through unified pipeline
      await processERNDelivery(ernContent, {
        feedId: feed.id,
        protocol: 'S3',
        source: `s3://${bucket}/${key}`,
        bucket,
        key
      });
      
      // Move to processed folder
      await this.moveToProcessed(bucket, key);
      
    } catch (error) {
      console.error('Error processing ERN:', error);
      await this.moveToError(bucket, key, error);
      throw error;
    }
  }
  
  async processAsset(bucket, key, feed) {
    // Store asset reference for later processing
    await storeAssetReference({
      feedId: feed.id,
      protocol: 'S3',
      location: {
        bucket,
        key
      },
      metadata: {
        size: record.s3.object.size,
        etag: record.s3.object.eTag
      }
    });
  }
  
  // Generate pre-signed URLs for distributors
  async generatePresignedUrl(operation, key, expiresIn = 3600) {
    const command = operation === 'upload' 
      ? new PutObjectCommand({ Bucket: this.bucket, Key: key })
      : new GetObjectCommand({ Bucket: this.bucket, Key: key });
    
    return getSignedUrl(this.s3Client, command, { expiresIn });
  }
  
  async moveToProcessed(bucket, key) {
    // Move file to processed folder
    const processedKey = key.replace(this.prefix, `${this.prefix}processed/`);
    await this.copyObject(bucket, key, processedKey);
    await this.deleteObject(bucket, key);
  }
  
  async moveToError(bucket, key, error) {
    // Move file to error folder with error details
    const errorKey = key.replace(this.prefix, `${this.prefix}errors/`);
    await this.copyObject(bucket, key, errorKey);
    await this.putErrorDetails(bucket, `${errorKey}.error`, error);
    await this.deleteObject(bucket, key);
  }
}

// Lambda function entry point
exports.handler = async (event, context) => {
  const config = await getS3Config();
  const listener = new S3DeliveryListener(config);
  
  try {
    await listener.handleS3Event(event);
    return {
      statusCode: 200,
      body: JSON.stringify({ 
        message: 'Successfully processed S3 event',
        records: event.Records.length 
      })
    };
  } catch (error) {
    console.error('Lambda execution error:', error);
    throw error;
  }
};
```

#### Google Cloud Storage Integration
```javascript
// GCS listener for deliveries
class GCSDeliveryListener {
  constructor(config) {
    this.bucket = storage.bucket(config.bucket);
    this.prefix = config.prefix;
    
    this.setupTriggers();
  }
  
  setupTriggers() {
    // Cloud Function triggered by file upload
    exports.processGCSDelivery = functions.storage
      .bucket(this.bucket.name)
      .object()
      .onFinalize(async (object) => {
        if (!object.name.startsWith(this.prefix)) return;
        
        // Identify the delivery feed
        const feed = await identifyFeedFromPath(object.name);
        if (!feed) return;
        
        // Process the delivery
        if (object.name.endsWith('.xml')) {
          await this.processERN(object, feed);
        } else {
          await this.processAsset(object, feed);
        }
      });
  }
  
  async processERN(object, feed) {
    const file = await this.bucket.file(object.name).download();
    const ern = file.toString();
    
    await processERNDelivery(ern, {
      feedId: feed.id,
      protocol: 'GCS',
      source: object.name
    });
  }
}
```

#### Unified Processing Pipeline
```javascript
// Common processing for all protocols
class UnifiedIngestionProcessor {
  async processDelivery(content, metadata) {
    const delivery = await this.createDeliveryRecord(metadata);
    
    try {
      // 1. Parse ERN
      const ern = await this.parseERN(content);
      
      // 2. Validate via Workbench
      const validation = await this.validateERN(ern);
      if (!validation.valid) {
        throw new ValidationError(validation.errors);
      }
      
      // 3. Process release
      const release = await this.processRelease(ern);
      
      // 4. Link assets
      await this.linkAssets(release, metadata.feedId);
      
      // 5. Send acknowledgment
      await this.sendAcknowledgment(delivery, release);
      
      // 6. Update feed status
      await this.updateFeedStatus(metadata.feedId, 'success');
      
    } catch (error) {
      await this.handleError(delivery, error);
      await this.updateFeedStatus(metadata.feedId, 'error', error);
    }
  }
  
  async validateERN(ern) {
    return workbenchAPI.validate({
      content: ern,
      type: 'ERN',
      version: this.detectVersion(ern),
      profile: this.detectProfile(ern)
    });
  }
}
```

## Data Models (Enhanced)

### Firestore Collections

```typescript
// delivery_feeds collection
interface DeliveryFeed {
  id: string;
  name: string;
  distributorId: string;
  distributorName: string;
  protocol: 'FTP' | 'SFTP' | 'S3' | 'GCS';
  
  credentials: {
    ftp?: {
      username: string;
      passwordHash: string;
      directory: string;
      ipWhitelist?: string[];
    };
    sftp?: {
      username: string;
      authType: 'password' | 'key';
      passwordHash?: string;
      publicKeys?: string[];
      directory: string;
    };
    s3?: {
      bucket: string;
      region: string;
      prefix: string;
      authType: 'iam-role' | 'access-keys' | 'signed-url';
      roleArn?: string;
      externalId?: string;
      accessKeyIdHash?: string;
      secretAccessKeyEncrypted?: encrypted;
      lambdaArn?: string;
    };
    gcs?: {
      serviceAccountEmail?: string;
      bucket: string;
      prefix: string;
      signedUrlSecret?: encrypted;
    };
  };
  
  settings: {
    autoProcess: boolean;
    validateBeforeProcess: boolean;
    requireAcknowledgment: boolean;
    notificationEmail?: string;
    maxFileSize: number; // MB
    allowedFileTypes: string[];
  };
  
  monitoring: {
    active: boolean;
    lastActivity?: Timestamp;
    lastSuccessfulDelivery?: Timestamp;
    lastError?: {
      timestamp: Timestamp;
      message: string;
      details: any;
    };
    statistics: {
      totalDeliveries: number;
      successfulDeliveries: number;
      failedDeliveries: number;
      averageProcessingTime: number; // seconds
    };
  };
  
  created: Timestamp;
  updated: Timestamp;
  createdBy: string;
}

// delivery_logs collection
interface DeliveryLog {
  id: string;
  feedId: string;
  protocol: 'FTP' | 'SFTP' | 'S3' | 'GCS';
  
  connection: {
    timestamp: Timestamp;
    clientIp?: string;
    username?: string;
    protocol: string;
    port?: number;
    // S3-specific fields
    bucket?: string;
    region?: string;
    eventType?: 's3:ObjectCreated:*' | 's3:ObjectRemoved:*';
  };
  
  files: Array<{
    name: string;
    size: number;
    type: string;
    uploadedAt: Timestamp;
    path: string;
    // S3-specific fields
    etag?: string;
    versionId?: string;
    storageClass?: string;
  }>;
  
  processing: {
    startedAt?: Timestamp;
    completedAt?: Timestamp;
    status: 'pending' | 'processing' | 'completed' | 'failed';
    ernValidation?: ValidationResult;
    errors?: string[];
    // Lambda-specific fields
    lambdaRequestId?: string;
    lambdaDuration?: number;
    lambdaMemoryUsed?: number;
  };
  
  result?: {
    releaseId?: string;
    trackCount?: number;
    acknowledgmentSent?: boolean;
  };
}

// Enhanced deliveries collection
interface Delivery {
  id: string;
  feedId: string;
  protocol: 'FTP' | 'SFTP' | 'S3' | 'GCS';
  sender: string;
  
  package: {
    originalPath: string;
    size: number;
    files: string[];
    // S3-specific
    s3Location?: {
      bucket: string;
      keys: string[];
    };
  };
  
  ern: {
    messageId: string;
    version: string;
    releaseCount: number;
  };
  
  processing: {
    receivedAt: Timestamp;
    startedAt?: Timestamp;
    completedAt?: Timestamp;
    status: 'pending' | 'processing' | 'completed' | 'failed';
    errors?: ProcessingError[];
  };
  
  acknowledgment?: {
    sentAt: Timestamp;
    messageId: string;
    method: string;
  };
}
```

## CLI Commands (Enhanced)

```bash
# Configure delivery protocols
ddex-dsp protocols setup         # Interactive protocol setup
ddex-dsp protocols list          # List configured protocols
ddex-dsp protocols test <type>   # Test specific protocol

# FTP/SFTP server management
ddex-dsp servers start           # Start all configured servers
ddex-dsp servers stop            # Stop all servers
ddex-dsp servers status          # Check server status
ddex-dsp servers logs <type>     # View server logs

# S3-specific commands
ddex-dsp s3 setup                # Interactive S3 setup wizard
ddex-dsp s3 deploy-lambda        # Deploy Lambda function
ddex-dsp s3 update-lambda        # Update Lambda code
ddex-dsp s3 test-trigger         # Test S3 trigger
ddex-dsp s3 create-bucket        # Create S3 bucket with proper configuration
ddex-dsp s3 configure-cors       # Setup CORS for bucket
ddex-dsp s3 generate-policy      # Generate bucket policy

# Delivery feed management
ddex-dsp feeds create            # Create new delivery feed
ddex-dsp feeds list              # List all feeds
ddex-dsp feeds test <id>         # Test specific feed
ddex-dsp feeds disable <id>      # Disable a feed

# Testing deliveries
ddex-dsp test-delivery \
  --protocol=s3 \
  --bucket=my-dsp-deliveries \
  --region=us-east-1 \
  --feed-id=<id> \
  --ern=test/sample.xml \
  --assets=test/assets/

# Lambda management
ddex-dsp lambda logs             # View Lambda function logs
ddex-dsp lambda metrics          # View Lambda metrics
ddex-dsp lambda test-event       # Send test S3 event
```

## Security & Isolation

### Protocol Security
```javascript
// Secure credential management with S3 support
class CredentialManager {
  async storeCredentials(feedId, protocol, credentials) {
    const encrypted = await this.encrypt(credentials);
    
    // Special handling for S3 credentials
    if (protocol === 'S3' && credentials.accessKeyId) {
      encrypted.accessKeyIdHash = this.hash(credentials.accessKeyId);
      delete encrypted.accessKeyId; // Never store raw access key
    }
    
    await firestore.collection('delivery_feeds').doc(feedId).update({
      [`credentials.${protocol}`]: encrypted,
      'updated': serverTimestamp()
    });
  }
  
  async validateAccess(protocol, authInfo) {
    // Find matching feed
    const feed = await this.findFeedByAuth(protocol, authInfo);
    if (!feed) return false;
    
    // Validate credentials
    switch(protocol) {
      case 'FTP':
      case 'SFTP':
        return this.validatePassword(feed, authInfo.password);
      case 'S3':
        return this.validateS3Access(feed, authInfo);
      case 'GCS':
        return this.validateServiceAccount(feed, authInfo.serviceAccount);
    }
  }
  
  async validateS3Access(feed, authInfo) {
    // S3 access is validated through IAM policies
    // This method checks if the Lambda execution role has access
    try {
      const s3Client = new S3Client({
        region: feed.credentials.s3.region,
        credentials: await this.getS3Credentials(feed)
      });
      
      // Test access with HeadBucket
      await s3Client.send(new HeadBucketCommand({
        Bucket: feed.credentials.s3.bucket
      }));
      
      return true;
    } catch (error) {
      console.error('S3 access validation failed:', error);
      return false;
    }
  }
}
```

### S3 Bucket Security
```javascript
// S3 bucket policy generator
class S3SecurityManager {
  generateBucketPolicy(bucket, feedId, lambdaArn) {
    return {
      Version: '2012-10-17',
      Statement: [
        {
          Sid: 'AllowLambdaRead',
          Effect: 'Allow',
          Principal: {
            Service: 'lambda.amazonaws.com'
          },
          Action: [
            's3:GetObject',
            's3:GetObjectVersion'
          ],
          Resource: `arn:aws:s3:::${bucket}/*`,
          Condition: {
            ArnEquals: {
              'aws:SourceArn': lambdaArn
            }
          }
        },
        {
          Sid: 'AllowDistributorUpload',
          Effect: 'Allow',
          Principal: {
            AWS: `arn:aws:iam::*:root`
          },
          Action: [
            's3:PutObject',
            's3:PutObjectAcl'
          ],
          Resource: `arn:aws:s3:::${bucket}/deliveries/${feedId}/*`,
          Condition: {
            StringEquals: {
              's3:x-amz-server-side-encryption': 'AES256'
            }
          }
        }
      ]
    };
  }
  
  generateLambdaExecutionRole() {
    return {
      Version: '2012-10-17',
      Statement: [
        {
          Effect: 'Allow',
          Action: [
            'logs:CreateLogGroup',
            'logs:CreateLogStream',
            'logs:PutLogEvents'
          ],
          Resource: 'arn:aws:logs:*:*:*'
        },
        {
          Effect: 'Allow',
          Action: [
            's3:GetObject',
            's3:DeleteObject',
            's3:PutObject'
          ],
          Resource: 'arn:aws:s3:::*-deliveries/*'
        },
        {
          Effect: 'Allow',
          Action: [
            'secretsmanager:GetSecretValue'
          ],
          Resource: 'arn:aws:secretsmanager:*:*:secret:ddex-dsp-*'
        }
      ]
    };
  }
}
```

### Delivery Isolation
```javascript
// Isolated delivery processing with S3 support
class IsolatedProcessor {
  async processDelivery(feedId, files, protocol) {
    // Create isolated workspace
    const workspace = await this.createWorkspace(feedId);
    
    try {
      // Protocol-specific file retrieval
      if (protocol === 'S3') {
        await this.downloadFromS3(files, workspace);
      } else {
        await this.moveFilesToWorkspace(files, workspace);
      }
      
      // Common processing
      await this.scanForThreats(workspace);
      await this.validateFiles(workspace);
      await this.processERN(workspace);
      
      // Clean up S3 files after processing
      if (protocol === 'S3') {
        await this.moveS3ToProcessed(files);
      }
      
    } finally {
      // Clean up workspace
      await this.cleanupWorkspace(workspace);
    }
  }
  
  async downloadFromS3(s3Files, workspace) {
    const s3Client = new S3Client({ region: s3Files[0].region });
    
    for (const file of s3Files) {
      const command = new GetObjectCommand({
        Bucket: file.bucket,
        Key: file.key
      });
      
      const response = await s3Client.send(command);
      const localPath = path.join(workspace, path.basename(file.key));
      
      await pipeline(
        response.Body,
        fs.createWriteStream(localPath)
      );
    }
  }
}
```

## Testing Features

### Protocol Testing Suite
```javascript
// Comprehensive protocol testing including S3
class ProtocolTestSuite {
  async testFTP(config) {
    const tests = [
      this.testConnection,
      this.testAuthentication,
      this.testFileUpload,
      this.testLargeFile,
      this.testMultipleFiles,
      this.testERNProcessing
    ];
    
    const results = [];
    for (const test of tests) {
      results.push(await test('FTP', config));
    }
    
    return {
      protocol: 'FTP',
      success: results.every(r => r.passed),
      results
    };
  }
  
  async testSFTP(config) {
    // Similar tests for SFTP
  }
  
  async testS3(config) {
    const tests = [
      this.testBucketAccess,
      this.testObjectUpload,
      this.testLambdaTrigger,
      this.testLargeFileUpload,
      this.testMultipartUpload,
      this.testSignedUrlGeneration,
      this.testEventProcessing,
      this.testErrorHandling
    ];
    
    const results = [];
    for (const test of tests) {
      results.push(await test('S3', config));
    }
    
    return {
      protocol: 'S3',
      success: results.every(r => r.passed),
      results
    };
  }
  
  async testGCS(config) {
    // Similar tests for GCS
  }
  
  // S3-specific test methods
  async testLambdaTrigger(protocol, config) {
    // Upload test file
    const testKey = `${config.prefix}test-${Date.now()}.xml`;
    await this.uploadTestFile(config.bucket, testKey);
    
    // Wait for Lambda execution
    await this.waitForProcessing(5000);
    
    // Check if file was processed
    const processed = await this.checkProcessedFolder(config.bucket, testKey);
    
    return {
      test: 'Lambda Trigger',
      passed: processed,
      details: processed ? 'Lambda triggered successfully' : 'Lambda did not process file'
    };
  }
  
  async testMultipartUpload(protocol, config) {
    const largeFile = this.generateLargeTestFile(100); // 100MB
    
    const upload = new Upload({
      client: new S3Client({ region: config.region }),
      params: {
        Bucket: config.bucket,
        Key: `${config.prefix}large-test.wav`,
        Body: largeFile
      },
      partSize: 10 * 1024 * 1024, // 10MB parts
      queueSize: 4
    });
    
    const result = await upload.done();
    
    return {
      test: 'Multipart Upload',
      passed: result.$metadata.httpStatusCode === 200,
      details: `Uploaded ${largeFile.size} bytes in ${upload.totalParts} parts`
    };
  }
}
```

### Integration with DDEX Distro
```javascript
// Seamless testing between Distro and DSP with S3
class DistroIntegrationTest {
  async testEndToEnd() {
    // 1. Create test release in DDEX Distro
    const release = await this.createTestRelease();
    
    // 2. Test each protocol
    const protocols = ['FTP', 'SFTP', 'S3', 'GCS'];
    const results = {};
    
    for (const protocol of protocols) {
      // Configure delivery in Distro
      await this.configureDistroDelivery(protocol);
      
      // Protocol-specific configuration
      if (protocol === 'S3') {
        await this.configureS3Delivery({
          bucket: 'test-dsp-deliveries',
          region: 'us-east-1',
          credentials: await this.getTestS3Credentials()
        });
      }
      
      // Send delivery
      const delivery = await this.sendDelivery(release, protocol);
      
      // Monitor reception in DSP
      const reception = await this.monitorReception(delivery.id);
      
      // Verify processing
      const processed = await this.verifyProcessing(release.id);
      
      results[protocol] = {
        sent: delivery.success,
        received: reception.success,
        processed: processed.success,
        time: processed.processingTime
      };
    }
    
    return results;
  }
}
```

## Lambda Deployment

### Serverless Configuration
```yaml
# lambda/s3-ingestion/serverless.yml
service: ddex-dsp-s3-ingestion

provider:
  name: aws
  runtime: nodejs18.x
  region: ${opt:region, 'us-east-1'}
  
  environment:
    FIREBASE_PROJECT_ID: ${env:FIREBASE_PROJECT_ID}
    FIREBASE_SERVICE_ACCOUNT: ${env:FIREBASE_SERVICE_ACCOUNT}
    WORKBENCH_API_KEY: ${env:WORKBENCH_API_KEY}
  
  iamRoleStatements:
    - Effect: Allow
      Action:
        - s3:GetObject
        - s3:DeleteObject
        - s3:PutObject
      Resource:
        - arn:aws:s3:::*-deliveries/*
    - Effect: Allow
      Action:
        - secretsmanager:GetSecretValue
      Resource:
        - arn:aws:secretsmanager:*:*:secret:ddex-dsp-*

functions:
  processDelivery:
    handler: index.handler
    timeout: 300 # 5 minutes
    memorySize: 1024
    events:
      - s3:
          bucket: ${opt:bucket}
          event: s3:ObjectCreated:*
          rules:
            - prefix: deliveries/
            - suffix: .xml
          existing: true

plugins:
  - serverless-plugin-optimize
  - serverless-offline

custom:
  optimize:
    external: ['firebase-admin']
```

### Lambda Deployment Script
```javascript
// scripts/deploy-lambda.js
const { execSync } = require('child_process');
const fs = require('fs');
const path = require('path');

async function deployLambda(config) {
  console.log('ðŸš€ Deploying S3 Lambda function...');
  
  // 1. Prepare Lambda package
  const lambdaDir = path.join(__dirname, '../lambda/s3-ingestion');
  process.chdir(lambdaDir);
  
  // 2. Install dependencies
  console.log('ðŸ“¦ Installing dependencies...');
  execSync('npm ci --production', { stdio: 'inherit' });
  
  // 3. Deploy with Serverless
  console.log('âš¡ Deploying with Serverless Framework...');
  execSync(`serverless deploy --stage ${config.stage} --region ${config.region} --bucket ${config.bucket}`, {
    stdio: 'inherit',
    env: {
      ...process.env,
      FIREBASE_PROJECT_ID: config.firebaseProjectId,
      WORKBENCH_API_KEY: config.workbenchApiKey
    }
  });
  
  // 4. Get Lambda ARN
  const info = JSON.parse(
    execSync('serverless info --json', { encoding: 'utf8' })
  );
  
  const lambdaArn = info.functions[0].arn;
  console.log(`âœ… Lambda deployed: ${lambdaArn}`);
  
  // 5. Update Firestore with Lambda ARN
  await updateFirestoreConfig(config.feedId, lambdaArn);
  
  return { lambdaArn };
}

module.exports = { deployLambda };
```

## Implementation Roadmap (Updated)

### Phase 1: Foundation (Weeks 1-4)
- [ ] Create CLI scaffolding tool
- [ ] Set up package structure
- [ ] Design Firestore schema with delivery feeds
- [ ] Implement unified auth
- [ ] Create base UI components
- [ ] Setup Firebase project template

### Phase 2: Multi-Protocol Ingestion (Weeks 5-8)
- [ ] Build FTP server implementation
- [ ] Build SFTP server with SSH key support
- [ ] **Implement S3 listener with Lambda**
- [ ] **Create S3 bucket configuration tools**
- [ ] **Build Lambda deployment pipeline**
- [ ] Implement GCS listeners
- [ ] Create unified processing pipeline
- [ ] Build delivery feed management API
- [ ] Implement credential storage
- [ ] Add protocol testing framework

### Phase 3: Configuration UI (Weeks 9-10)
- [ ] Build delivery configuration interface
- [ ] Create protocol selection wizard
- [ ] **Add S3 configuration UI**
- [ ] **Implement Lambda setup wizard**
- [ ] **Add IAM role configuration helper**
- [ ] Implement credential generation
- [ ] Add test delivery functionality
- [ ] Build monitoring dashboard
- [ ] Create activity feed

### Phase 4: Core Streaming (Weeks 11-14)
- [ ] Implement catalog structure
- [ ] Build streaming API
- [ ] Add HLS/DASH support
- [ ] Create web player
- [ ] Implement basic search
- [ ] Add user library

### Phase 5: Testing & Optimization (Weeks 15-16)
- [ ] **S3 integration testing**
- [ ] **Lambda performance optimization**
- [ ] **Multi-region S3 testing**
- [ ] Protocol comparison benchmarks
- [ ] Load testing with all protocols

### Phase 6: Analytics & Reporting (Weeks 17-20)
- [ ] Implement play tracking
- [ ] Build analytics dashboard
- [ ] Create DSR generator
- [ ] Add usage reports
- [ ] **S3 cost analysis tools**
- [ ] Implement billing integration
- [ ] Add admin panel

### Phase 7: Advanced Features (Weeks 21-24)
- [ ] Add recommendation engine
- [ ] Implement offline playback
- [ ] Add podcast support
- [ ] Create artist tools
- [ ] Build mobile apps
- [ ] Add live streaming

### Phase 8: Testing & Launch (Weeks 25-28)
- [ ] Comprehensive testing
- [ ] Performance optimization
- [ ] Security audit
- [ ] Documentation
- [ ] Demo deployment
- [ ] npm publication

## Success Metrics

### Protocol Performance Targets
- **FTP Transfer Speed**: >10MB/s
- **SFTP Transfer Speed**: >8MB/s
- **S3 Transfer Speed**: >100MB/s (leveraging AWS backbone)
- **GCS Transfer Speed**: >50MB/s
- **Protocol Switching**: <5 seconds
- **Concurrent Feeds**: Support 100+ active feeds
- **Lambda Cold Start**: <2 seconds
- **Lambda Processing Time**: <10 seconds for typical ERN

### Testing Metrics
- **Protocol Coverage**: 100% of protocols testable
- **Test Execution**: <30 seconds per protocol
- **Integration Success**: 95%+ with DDEX Distro
- **S3 Reliability**: 99.99% (AWS SLA)

## Cost Considerations

### S3 Cost Optimization
```javascript
// S3 lifecycle rules for cost optimization
const lifecycleRules = {
  transitions: [
    {
      days: 30,
      storageClass: 'STANDARD_IA' // Infrequent Access
    },
    {
      days: 90,
      storageClass: 'GLACIER_IR' // Glacier Instant Retrieval
    }
  ],
  expiration: {
    days: 365 // Delete after 1 year
  },
  noncurrentVersionExpiration: {
    days: 7 // Delete old versions after 7 days
  }
};

// Lambda cost monitoring
const costMonitor = {
  estimateMonthlyCost: (invocations, avgDuration, memorySize) => {
    const requests = invocations * 0.0000002; // $0.20 per 1M requests
    const compute = invocations * (avgDuration / 1000) * (memorySize / 1024) * 0.0000166667;
    return { requests, compute, total: requests + compute };
  }
};
```

## Future Enhancements

### Advanced S3 Features (v2.0)
1. **S3 Intelligent-Tiering**: Automatic storage class optimization
2. **S3 Batch Operations**: Bulk processing of historical deliveries
3. **S3 Event Notifications**: SNS/SQS integration for complex workflows
4. **Cross-Region Replication**: Global delivery support
5. **S3 Object Lock**: Compliance mode for regulatory requirements

### Platform Extensions
1. **CloudFront Integration**: CDN for global streaming
2. **AWS Elemental MediaConvert**: Adaptive bitrate encoding
3. **Amazon Rekognition**: Content moderation
4. **AWS Step Functions**: Complex workflow orchestration
5. **Amazon EventBridge**: Event-driven architecture

## Technical Considerations

### S3-Specific Optimizations
```javascript
// S3 Transfer Acceleration
const acceleratedUpload = {
  endpoint: 'https://my-bucket.s3-accelerate.amazonaws.com',
  benefits: [
    'Up to 50% faster uploads',
    'Optimized routing',
    'Edge location uploads'
  ]
};

// Multipart upload optimization
const multipartConfig = {
  partSize: 10 * 1024 * 1024, // 10MB parts
  queueSize: 4, // Parallel parts
  leavePartsOnError: false,
  tags: {
    'delivery-feed': feedId,
    'content-type': 'ddex-ern'
  }
};
```

### Monitoring
```javascript
// Comprehensive monitoring including S3/Lambda
import { monitor } from '@ddex/monitoring';

monitor.track('ingestion', {
  protocol: 's3',
  success: ingestSuccess,
  duration: processingTime,
  lambdaDuration: context.getRemainingTimeInMillis(),
  memoryUsed: process.memoryUsage().heapUsed,
  releaseId,
  sender
});

monitor.alert('lambda-errors', {
  threshold: 10,
  window: '5m',
  action: 'page-oncall'
});

monitor.cost('s3-usage', {
  requests: s3RequestCount,
  storage: s3StorageGB,
  transfer: s3TransferGB
});
```

## Getting Started

### Quick Start
```bash
# Install CLI
npm install -g @ddex/dsp-cli

# Create your streaming platform with S3 support
ddex-dsp create my-platform --template=streaming --protocols=s3,ftp,sftp

# Deploy with S3 configuration
cd my-platform
ddex-dsp s3 setup        # Interactive S3 setup
ddex-dsp deploy           # Deploy platform and Lambda

# Your DSP is live with S3 support! ðŸŽµ
```

### Test S3 Delivery with DDEX Distro
```bash
# Configure S3 delivery in DDEX Distro
ddex-distro delivery configure \
  --protocol=s3 \
  --bucket=my-dsp-deliveries \
  --region=us-east-1 \
  --credentials=~/.aws/credentials

# Send test delivery to S3
ddex-distro deliver \
  --protocol=s3 \
  --release=test-album

# Monitor in DSP
ddex-dsp deliveries list --protocol=s3
ddex-dsp lambda logs --tail
```

### Next Steps
1. Configure your brand
2. Set up S3 buckets and Lambda functions
3. Test with multiple protocols
4. Customize the interface
5. Launch to the world

The future of music streaming is open, compliant, and yours to build - now with the power and scale of Amazon S3.